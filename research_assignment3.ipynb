{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Discussion Assignment 3\n",
    "\n",
    "DATA 612 \n",
    "Michael D'Acampora\n",
    "\n",
    "As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to working together with machines we get to understand how nuanced our prediction making can be. Recommender systems (recsys) have been useful especially in the ecommerce and media industries, but they also have had negative effects and require quick but thoughtful changes in tuning.\n",
    "\n",
    "The first recsys that comes to mind is what Youtube employs. Since there are billions of views every day, a robust system had to be created to provide recommendations to videos a viewer could never find but likely enjoy. For the most part it has been a success. For example, video game viewers can get loads of gamer clips which can give the viewer hours of content they wouldn't otherwise watch. What has been a downside for the system is when someone begins to watch conspiracy theory videos or other extreme propoganda. The system then continues to recommend an onslaught of similar, propogandized content which sends the viewer down a path of fallacy and misinformation, which they may or may not act on. We have seen people show up at rallys that were perpetrated by fake political propaganda, and others create viral videos of their own based on hateful misinformation they had been viewing on their own.\n",
    "\n",
    "Another scary topic of recent is digital ethics and prejudice. [This video](https://youtu.be/MqoRzNhrTnQ?t=696) tells a story about how one particular system pointed people criminal rehabilitation ads to people with names of African origin. Examples like this can be found in a number of places which and brings to the forefront the idea that a) these algorithms must constantly be tuned to avoid these types of ethical issues, and b) more minorities in tech can help make the industry and user experience better at version 1.0. These pitfalls are seen with gender stereotyping and sexism as well. In a study where fake profiles were made and send around the web and tracked, the only difference between the profiles was whether it was a man or woman, the women's profiles were less likely to be shown ads for high-paid jobs on Google. \n",
    "\n",
    "A number of techincal approaches can be used to minimize or even eliminate some of these issues. [Diversity Controlled Testing](https://youtu.be/MqoRzNhrTnQ?t=1089) is one method that can help a developer see if it treats profiles differently. But it starts with the development team. Approaching algorithm buildilng with ethics and empathy in mind is the first step towards building a better internet.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
