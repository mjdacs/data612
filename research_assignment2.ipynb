{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Discussion Assignment 2\n",
    "\n",
    "DATA 612 \n",
    "Michael D'Acampora\n",
    "\n",
    "### Music Recommendations at Scale with Spark\n",
    "For this discussion item we summarized what we found to be the most important or interesting points regading the talk given by Christopher Johnson from Spotify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Interesing Points:\n",
    "\n",
    "A large part of what made Pandora's Music Genome Project was manually tagging attributes to songs.\n",
    "\n",
    "Netflix's old system (2014) used matrix factorization to try to predict what a user would rate a certain movie that has not been watched by them. It reduced dimensionality and linear algebra to determine a new rating.\n",
    "\n",
    "In Spotify's case they try to implicity infer what a user would like. They take the same idea but instead of user ratings, values are binary; 0 for not streamed, 1 for streamed. A loss function (confidence) is weigthed by number of times streamed. Alternating least squares regression is used.\n",
    "\n",
    "In 2009 Spotify had just a small room that housed a few servers, now they have a large data center in London. \n",
    "\n",
    "The Spotify team started using Hadoop for distributed computing to help process the enormous data they were collecting. It worked well at first but they realized there was a repetitive read/write task on every iteration of a job. There were efficiencies to be improved upon doing business this way.\n",
    "\n",
    "In comes Spark, where the data can be loaded into memory just once, and iterations can be performed there, instead of reading/writing from disk every iteration. A lot less resources are used for every opertaion because one can subset the data to memory as well. After learning Spark and getting better with using it, the team ran a test job on Hadoop and Spark which reduced the time from 10 hours (Hadoop) to 1.5 hours (Spark).\n",
    "\n",
    "Spark is written in Scala to be run on the Java Virtual Machine. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
