{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://deborahs-air:4040\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1562354452959)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 2\n",
       "y: Int = 3\n",
       "res0: Int = 5\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = 2\n",
    "val y = 3\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.SparkContext._\n",
       "import org.apache.log4j._\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.mllib.recommendation.ALS\n",
       "import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
       "import org.apache.spark.mllib.recommendation.Rating\n",
       "import scala.Tuple2\n",
       "import org.apache.spark.rdd.RDD\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.log4j._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "import scala.Tuple2\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final case class Movie(movieID: Int, userID: Int, rating: int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@35f99d89\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"MovieLens Recommendation\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsFile: String = /Users/deborahgemellaro/Programming/612/project_5/ml-latest-small/ratings.csv\n",
       "moviesFile: String = /Users/deborahgemellaro/Programming/612/project_5/ml-latest-small/movies.csv\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val ratingsFile = spark.sparkContext.textFile(\"/Users/deborahgemellaro/Programming/612/project_5/ml-latest-small/ratings.csv\")\n",
    "val ratingsFile = \"/Users/deborahgemellaro/Programming/612/project_5/ml-latest-small/ratings.csv\"\n",
    "val moviesFile = \"/Users/deborahgemellaro/Programming/612/project_5/ml-latest-small/movies.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "65: error: value toDF is not a member of String",
     "output_type": "error",
     "traceback": [
      "<console>:65: error: value toDF is not a member of String",
      "       val ratingsDF = ratingsFile.toDF()",
      "                                   ^",
      ""
     ]
    }
   ],
   "source": [
    "// Convert to a dataframe\n",
    "import spark.implicits._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [userId: string, movieId: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", true).load(ratingsFile)\n",
    "df1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "|     1|    349|   4.0|964982563|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ratingsDF: org.apache.spark.sql.DataFrame = [userId: string, movieId: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratingsDF = df1.select(df1.col(\"userId\"), \n",
    "                           df1.col(\"movieId\"), \n",
    "                           df1.col(\"rating\"), \n",
    "                           df1.col(\"timestamp\"))\n",
    "ratingsDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [movieId: string, title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", true).load(moviesFile)\n",
    "df2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "moviesDF: org.apache.spark.sql.DataFrame = [movieId: string, title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviesDF = df2.select(df2.col(\"movieId\"), \n",
    "                          df2.col(\"title\"),\n",
    "                          df2.col(\"genres\"))\n",
    "moviesDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Register both DataFrames as temp tables to make querying easier\n",
    "ratingsDF.createOrReplaceTempView(\"ratings\")\n",
    "moviesDF.createOrReplaceTempView(\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: Long = 100836\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Count total number of ratings\n",
    "ratingsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: Long = 610\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Count number of distinct users\n",
    "ratingsDF.select(ratingsDF.col(\"userId\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res31: Long = 9724\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Count number of distinct movies\n",
    "ratingsDF.select(ratingsDF.col(\"movieId\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": " error: incomplete input",
     "output_type": "error",
     "traceback": [
      "<console>: error: incomplete input"
     ]
    }
   ],
   "source": [
    "val results - spark.sql(\"SELECT movies.title, movierates.maxr, movierates.minr, movierates.cntu FROM(SELECT ratings.movieId, MAX(ratings.rating) as maxr, MIN(ratings.rating) as minr, COUNT(DISTINCT userId) as cntu FROM ratings GROUP BY ratings.movieId) movierates JOIN movies on movierates.movieId=movies.movieId ORDER BY movierates.cntu DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------------------+\n",
      "|userId|movieID|rating|               title|\n",
      "+------+-------+------+--------------------+\n",
      "|   345|    779|   5.0|'Til There Was Yo...|\n",
      "|   345|    838|   5.0|         Emma (1996)|\n",
      "|   345|    926|   5.0|All About Eve (1950)|\n",
      "|   345|    932|   5.0|Affair to Remembe...|\n",
      "|   345|   1203|   5.0| 12 Angry Men (1957)|\n",
      "|   345|   1214|   5.0|        Alien (1979)|\n",
      "|   345|   2359|   5.0|Waking Ned Devine...|\n",
      "|   345|   2971|   5.0|All That Jazz (1979)|\n",
      "|   345|   7121|   5.0|   Adam's Rib (1949)|\n",
      "|   345|  27751|   5.0| 'Salem's Lot (2004)|\n",
      "+------+-------+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newTable: org.apache.spark.sql.DataFrame = [userId: string, movieID: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newTable = spark.sql(\"SELECT ratings.userId, ratings.movieID, ratings.rating, movies.title FROM ratings JOIN movies ON movies.movieId=ratings.movieId WHERE ratings.userId=345 and ratings.rating > 4\")\n",
    "                        \n",
    "newTable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 75834 Test: 25002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "splits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([userId: string, movieId: string ... 2 more fields], [userId: string, movieId: string ... 2 more fields])\n",
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userId: string, movieId: string ... 2 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userId: string, movieId: string ... 2 more fields]\n",
       "numTraining: Long = 75834\n",
       "numTest: Long = 25002\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = ratingsDF.randomSplit(Array(0.75, 0.25), seed = 123L)\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "val numTraining = trainingData.count()\n",
    "val numTest = testData.count()\n",
    "println(\"Training: \" + numTraining + \" Test: \" + numTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[256] at map at <console>:61\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratingsRDD = trainingData.rdd.map(row => {\n",
    "    val userId = row.getString(0)\n",
    "    val movieId = row.getString(1)\n",
    "    val ratings = row.getString(2)\n",
    "    Rating(userId.toInt, movieId.toInt, ratings.toDouble)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ALS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
