{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.172:4040\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1562514739602)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.SparkContext._\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.mllib.recommendation.Rating\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.ml.recommendation.ALS\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.recommendation.ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5 - Implementing a Recommender System on Spark\n",
    "\n",
    "#### The goal of this project is give you practice beginning to work with a distributed recommender system.  It is sufficient for this assignment to build out your application on a single node.\n",
    "\n",
    "I chose to give myself a challenge and create the recommender system in Spark Scala. Since Spark is written natively in Scala I figured it would be good to understand how it works at the base level before understaning the Python/R Spark frameworks.\n",
    "\n",
    "What is nice is that you can create a Scala kernel in Jupyter notebook which I did here. There is a link in the references below that instructs you how to do it. Additionally I included the JVM .scala file in my git repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a SparkSession on our local disk to get things started. With the master set as local it will only run jobs on your own computer but \"distribute\" the work across however many cores you have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@240223d0\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"MovieLens Recommendation\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsFile: String = /Users/deborahgemellaro/Programming/612/project_5/ratings.csv\n",
       "moviesFile: String = /Users/deborahgemellaro/Programming/612/project_5/movies.csv\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratingsFile = \"/Users/deborahgemellaro/Programming/612/project_5/ratings.csv\"\n",
    "val moviesFile = \"/Users/deborahgemellaro/Programming/612/project_5/movies.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create DataFrames from our .csv files. At the base level Spark has what are called Resilient Distributed Datasets, or RDDs. While this data structure was the most widely used, the Spark SQL library introduced DataSet and DataFrame structures which are rapidly becoming the data structure of choice in the industry. They behave much more like DataFrames in R and Python which allow you to perform SQL-like transformations on them. \n",
    "\n",
    "At first I pulled the csv in as an RDD and coverted to a DataFrame. But there is a DataFrame reader and we use that here to create `df1` and `df2` for ratings and movies, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [userId: string, movieId: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.read.format(\"csv\")\n",
    "    .option(\"header\", true)\n",
    "    .load(ratingsFile)\n",
    "\n",
    "df1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "|     1|    349|   4.0|964982563|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ratingsDF: org.apache.spark.sql.DataFrame = [userId: string, movieId: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratingsDF = df1.select(df1.col(\"userId\"), \n",
    "                           df1.col(\"movieId\"), \n",
    "                           df1.col(\"rating\"), \n",
    "                           df1.col(\"timestamp\"))\n",
    "ratingsDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [movieId: string, title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = spark.read.format(\"csv\")\n",
    "    .option(\"header\", true)\n",
    "    .load(moviesFile)\n",
    "\n",
    "df2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "moviesDF: org.apache.spark.sql.DataFrame = [movieId: string, title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviesDF = df2.select(df2.col(\"movieId\"), \n",
    "                          df2.col(\"title\"),\n",
    "                          df2.col(\"genres\"))\n",
    "moviesDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `createOrReplaceTempView` is one of the powerful things in Spark and it creates a temporary view of the DataFrame in order to run SQL queries. There are a few `dplyr` or SQL-like methods available as shown below that you can manipulate your DataFrame with, but if you need to run a SQL query you would do it on a temp view as shown in `val newTable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Register both DataFrames as temp tables to make querying easier\n",
    "ratingsDF.createOrReplaceTempView(\"ratings\")\n",
    "moviesDF.createOrReplaceTempView(\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 100836\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Count total number of ratings\n",
    "ratingsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Long = 610\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Count number of distinct users\n",
    "ratingsDF.select(ratingsDF.col(\"userId\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Long = 9724\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Count number of distinct movies\n",
    "ratingsDF.select(ratingsDF.col(\"movieId\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------------------+\n",
      "|userId|movieID|rating|               title|\n",
      "+------+-------+------+--------------------+\n",
      "|   345|    779|   5.0|'Til There Was Yo...|\n",
      "|   345|    838|   5.0|         Emma (1996)|\n",
      "|   345|    926|   5.0|All About Eve (1950)|\n",
      "|   345|    932|   5.0|Affair to Remembe...|\n",
      "|   345|   1203|   5.0| 12 Angry Men (1957)|\n",
      "|   345|   1214|   5.0|        Alien (1979)|\n",
      "|   345|   2359|   5.0|Waking Ned Devine...|\n",
      "|   345|   2971|   5.0|All That Jazz (1979)|\n",
      "|   345|   7121|   5.0|   Adam's Rib (1949)|\n",
      "|   345|  27751|   5.0| 'Salem's Lot (2004)|\n",
      "+------+-------+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newTable: org.apache.spark.sql.DataFrame = [userId: string, movieID: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newTable = spark.sql(\"SELECT ratings.userId, ratings.movieID, ratings.rating, movies.title FROM ratings JOIN movies ON movies.movieId=ratings.movieId WHERE ratings.userId=345 and ratings.rating > 4\")\n",
    "                        \n",
    "newTable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model\n",
    "\n",
    "Time to split the data up in to training and test sets. First we determine our ratio in the `splits` value, then since the DataFrames are all still string values, we convert the columns to the correct data types in `traningSet` and `testSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for -- Training Data: 75834, Test Data: 25002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "splits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([userId: string, movieId: string ... 2 more fields], [userId: string, movieId: string ... 2 more fields])\n",
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userId: string, movieId: string ... 2 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userId: string, movieId: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = ratingsDF.randomSplit(Array(0.75, 0.25), seed = 123L)\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "println(\"Counts for -- Training Data: \" + trainingData.count() + \", Test Data: \" + testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSet: org.apache.spark.sql.Dataset[org.apache.spark.mllib.recommendation.Rating] = [user: int, product: int ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSet = trainingData.map(row => {\n",
    "    val userId = row.getString(0)\n",
    "    val movieId = row.getString(1)\n",
    "    val ratings = row.getString(2)\n",
    "    Rating(userId.toInt, movieId.toInt, ratings.toDouble)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testSet: org.apache.spark.sql.Dataset[org.apache.spark.mllib.recommendation.Rating] = [user: int, product: int ... 1 more field]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testSet = testData.map(row => {\n",
    "    val userId = row.getString(0)\n",
    "    val movieId = row.getString(1)\n",
    "    val ratings = row.getString(2)\n",
    "    Rating(userId.toInt, movieId.toInt, ratings.toDouble)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model\n",
    "\n",
    "We are choosing choosing the Alternating Least Squares model since it is a model type that is known to work best on distributed networks. Spark has the `ALS()` API as part of their machine learning libraries. Once we set up the model we fit it to our `traningSet`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "als: org.apache.spark.ml.recommendation.ALS = als_36b96ee0fd0a\n",
       "model: org.apache.spark.ml.recommendation.ALSModel = als_36b96ee0fd0a\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val als = new ALS()\n",
    "  .setMaxIter(5)\n",
    "  .setRegParam(0.01)\n",
    "  .setUserCol(\"user\")\n",
    "  .setItemCol(\"product\")\n",
    "  .setRatingCol(\"rating\")\n",
    "\n",
    "val model = als.fit(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we run the `model` on the `testSet` using the `transform()` method in the `ALS()` class. There are some NaNs in this file so we drop them in order to obtain a numerical result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [user: int, product: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(testSet).na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model\n",
    "\n",
    "The Spark ML library has a `RegressionEvaluator()` API that allows you to evaluate your models across a number of different metrics. Here we score the model using RMSE, MSE, and MAE just for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 1.0986410985069273\n",
      "MSE = 1.2070122633285083\n",
      "MAE = 0.8289660242729547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluatorRMSE: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_5e8b152c1c9e\n",
       "evaluatorMSE: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_c3088d9c93fc\n",
       "evaluatorMAE: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_93b79dbca67d\n",
       "rmse: Double = 1.0986410985069273\n",
       "mse: Double = 1.2070122633285083\n",
       "mae: Double = 0.8289660242729547\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluatorRMSE = new RegressionEvaluator()\n",
    "    .setMetricName(\"rmse\")\n",
    "    .setLabelCol(\"rating\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "val evaluatorMSE = new RegressionEvaluator()\n",
    "    .setMetricName(\"mse\")\n",
    "    .setLabelCol(\"rating\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "val evaluatorMAE = new RegressionEvaluator()\n",
    "    .setMetricName(\"mae\")\n",
    "    .setLabelCol(\"rating\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "val rmse = evaluatorRMSE.evaluate(predictions)\n",
    "val mse = evaluatorMSE.evaluate(predictions)\n",
    "val mae = evaluatorMAE.evaluate(predictions)\n",
    "\n",
    "println(f\"RMSE = $rmse\")\n",
    "println(f\"MSE = $mse\")\n",
    "println(f\"MAE = $mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Always stop your Spark Session when finished \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "When comparing the MSE to that of a baseline Matrix Factorization model used in project 3, it appears there is a higher degree of error in the ALS model chosen here, which is very surprising to me. I would have to go back and have more control over the way the data is shaped to see why the results are the way they are. This is a case where I believe user error could be a factor.\n",
    "\n",
    "Overall, Scala is an interesting language and can see how using a functional paradigm is applied. Performance is reported to be a lot better than using Pyspark or R, but converting teams over to Scala may be time consuming and lose productivity at first.\n",
    "\n",
    "For a dataset like I've used here with 100k rows of movie ratings, Spark is not actually necessary, but as things get order of magnitude larger, one CPU will not do the job. That is when Spark comes in. It's really an excellent framework that \"magically\" manages the distributed job behind the scenes when connecting to a cluster of machines and at much less of a cost than its Hadoop predecessor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "How to run Scala and Spark in the Jupyter notebook\n",
    "\n",
    "https://medium.com/@bogdan.cojocar/how-to-run-scala-and-spark-in-the-jupyter-notebook-328a80090b3b\n",
    "\n",
    "\n",
    "Collaborative Filtering\n",
    "\n",
    "https://spark.apache.org/docs/2.0.2/ml-collaborative-filtering.html\n",
    "\n",
    "\n",
    "Scala Machine Learning Projects: Recommendation Systems\n",
    "\n",
    "https://medium.com/@navdeepsingh_2336/scala-machine-learning-projects-recommendation-systems-d41d9eebbb06\n",
    "\n",
    "\n",
    "Why Spark ML ALS algorithm print RMSE = NaN?\n",
    "\n",
    "https://stackoverflow.com/questions/43544815/why-spark-ml-als-algorithm-print-rmse-nan/47236423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
